\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{textcomp}
\usepackage{a4wide}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subfig}
\usepackage{listings}
\usepackage{hyperref}
%\usepackage{fontspec}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsthm}

\lstset{
language=Python,
basicstyle=\ttfamily\small,
otherkeywords={self},                   
}

\title{Title}
\title{Неточный метод Ньютона.}
\date{4 октября 2015}
\author{Павел Измаилов}

\begin{document}

\renewcommand{\contentsname}{\centerline{\bf Contents}}
\renewcommand{\refname}{\centerline{\bf Literature}}

\newcommand{\GP}{\mathcal{GP}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\cov}{\mbox{cov}}
\newcommand{\Nystrom}{Nystr\"{o}m }
\newcommand{\KL}[2]{\mbox{KL}\left(#1\mbox{ || }#2\right)}
\newcommand{\tr}{\mbox{tr}}
\newcommand{\derivative}[2]{\frac{\partial #1}{\partial #2}}

\newlength{\arrayrulewidthOriginal}
\newcommand{\Cline}[2]{%
  \noalign{\global\setlength{\arrayrulewidthOriginal}{\arrayrulewidth}}%
  \noalign{\global\setlength{\arrayrulewidth}{#1}}\cline{#2}%
  \noalign{\global\setlength{\arrayrulewidth}{\arrayrulewidthOriginal}}}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}


\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}} {\mbox{\boldmath$\scriptscriptstyle#1$}}}

%\maketitle
\centerline{Lomonosov Moscow State University}

\centerline{Faculty of Computer Science}

\vspace{5 cm}

\centerline{\Large Review of matherials on}

\vspace{1 cm}

\centerline{\Large \bf Gaussian Processes for Machine Learning}

\vspace{6 cm}

\begin{flushright}

Pavel Izmailov
\end{flushright}

\vfill 

\centerline{Moscow,  2016}
\thispagestyle{empty} 
\pagebreak

\section{Theory}

\hspace{0.6cm}In this section an introduction to Gaussian process theory is provided.

\subsection{Gaussian Process}

	\hspace{0.6cm}Consider the following definition
	\begin{definition}
		A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
	\end{definition}
	A Gaussian process is completely specified by it's mean function and covariance function. These functions are defined as follows
	\begin{definition}
		Let $f(x)$ be a real-valued Gaussian process. Then the functions
		$$m(x) = \E[f(x)],$$
		$$k(x, x') = \E[(f(x) - m(x)) (f(x') - m(x'))],$$
		are the mean function and the covariance function of the process $f$ respectively. 
	\end{definition}
	
	We will write the Gaussian process as $f(x) \sim \GP(m(x), k(x, x'))$.
	
\subsection{GP-regression}
	\hspace{0.6cm}Consider the following task. We have a dataset $\{(x_i, f_i) | i = 1, \ldots, n\}$, generated from a Gaussian process $f \sim \GP(m(x), k(x, x'))$, let $x \in \R^d$.  We will denote the matrix comprised of points $x_1, \ldots, x_n$ by $X \in \R^{n \times d}$ and the vector of corresponding values $f_1, ..., f_n$ by $f \in \R^n$. We want to predict the values $f_* \in \R^m$ of this random process at a set of other m points $X_* \in \R^{m \times d}$. The joint distribution of $f$ and $f_*$ is given by
	$$
	\left [ \begin{array}{c} f\\ f_* \end{array} \right ]
	\sim
	\N \left ( 0, \left [\begin{array}{cc} K(X, X) & K(X, X_*)\\ K(X_*, X) & K(X_*, X_*) \end{array} \right] \right ),
	$$
	where $K(X, X) \in \R^{n \times n}$, $K(X, X_*) = K(X^*, X)^T \in \R^{n \times m}$, $K(X^*, X^*) \in \R^{m \times m}$ are the matrices comprised of pairwise values of the covariance function $k$ for the given sets.
	
	The conditional distributin
	
	$$f_* | X_*, X, f \sim \N( \hat m, \hat K ),$$
	where 
	$$\E [f_* | f ] = \hat m = K(X_*, X) K(X, X)^{-1} f,$$
	$$\cov(f_* | f ) = \hat K = K(X_*, X_*) - K(X_*, X)K(X, X)^{-1}K(X, X_*).$$
		
	Thus, predicting the values of the Gaussian process at a new data point requires solving a linear system with a matrix of size $n \times n$ and thus scales as $O(n^3)$.
	
	\subsubsection{Noisy case}
		\hspace{0.6cm}Consider the following model. We now have a dataset $\{(x_i, y_i)| i = 1, \ldots n\}$, where $y_i = f(x_i) + \varepsilon$, $\varepsilon \sim \N(0, \sigma_n)$. This means that we only have access to the noisy observations and not the true values of the process at data points. With the notation and logics similar to the one we used it the previous section we can find the conditional distribution for the values $f_*$ of the process at new points $X_*$ in this case:
		$$f_* | y \sim \N( \hat m, \hat K ),$$
		$$\E[f_* | y] = \hat m = K(X_*, X) (K(X, X) + \sigma_n^2 I)^{-1} y,$$
		$$\cov(f_* | y ) = \hat K = K(X_*, X_*) - K(X_*, X)(K(X, X) + \sigma_n^2 I)^{-1}K(X, X_*).$$
		
\subsection{GP-classification}
	\hspace{0.6cm}To be wrritten.
	
\subsection{Kernel functions}
	\hspace{0.6cm}To be wrritten.
	
\subsection{Hyper-parameter estimation}
	\hspace{0.6cm}Bayesian paradigm provides a way of estimating the kernel hyper-parameters of the GP-model through th  maximizization of the marginal likelihood of the model. Marginal likelihood is given by
	$$p(y | X) = \int p(y | f, X) p(f | X) df,$$
	which is the likelihood, marginalized over the hidden values $f$ of the underlying process.
	
\subsection{Theoretical perspectives}
	\hspace{0.6cm}To be wrritten.

\pagebreak
\section{Review of existing methods}

\hspace{0.6cm}It follows from the discussion above, that full Gaussian process regression scales as $O(n^3)$ and thus cannot be applied to big datasets. In this section we will review several approximate methods, that make Gaussian processes practical.

\subsection{Methods, based on inducing inputs}
	\hspace{0.6cm}Most of the existing methods are based on introducing a set of $m$ function points that are called inducing inputs. Using these inputs one can make approximate predictions of the values of the hidden process at test points with a complexity of $O(nm^3)$ instead of $O(n^3)$.
	
	Consider the following situation. We have a dataset of $n$ examples $x_i$ with corresponding values $y_i$. We will denote the matrix of pairwise values of the covariance function by $K_{nn}$. Now we introduce a set of $m$ inducing inputs. We will denote the corresponding covariance matrix by $K_{mm}$ and the matrices of covariances between the inducing points and training points by $K_{nm}$ and $K_{mn}$. We will denote the vectors, comprised of noisy and true function values $y_i$ and $f_i$ at training points by $y$ and $f$ respectively. We will also introduce a distribution $q(u)$ over the hidden function values $u$ at the inducing inputs.
	
%	Most of the inducing point methods form a low-rank approximation $\tilde K$ to the covariance matrix $K_{nn}$ of the following form
%	$$\tilde K = K_{nm} K_{mm}^{-1} K_{mn}.$$
%	This approximation is called the \Nystrom approximation of the matrix $K_{nn}$.
	
%	The methods mostly differ in the way they use to find the inducing points. The most basic idea is to choose this points at random or by some criterion from the training data, or apply some clustering procedure and use the cluster centers as the inducing points. More advanced methods use variational inference (or it's stochastic variant) to find these points. In this section a description of several methods is be provided. 
	
	It's easy to see, that
	$$p(y|f) = \N (y|f, \sigma_n I),$$
	$$p(f|u) = \N (f|K_{nm} K_{mm}^{-1}u, \tilde K),$$
	$$p(u) = \N(u|0, K_{mm}),$$
	where $\tilde K = K_{nn} - K_{nm} K_{mm}^{-1} K_{mn}.$
		
	\subsubsection{Variational learning of inducing points}
		\label{Titsias}
		
		\hspace{0.6cm}The method discussed here was introduced in \cite{Titsias}. This method provides a way to find the optimal positions of the inducing points, as well as the optimal distribution of the process value at these points.

		Let $z$ denote a vector comprized of the process values at some new points. We can calculate the predictive distribution at these points as follows
		$$p(z|y) = \int p(z|f) p(f|y) df.$$
		Let's fix the inducing point positions $x_1, \ldots, x_m$. As above, $u$ is the vector compised of the process values at these points. We can rewrite the above equation
		\begin{equation}
			\label{predictive1}
			p(z|y) = \iint p(z|u, f) p(f| u, y) p(u|y)df du,
		\end{equation}
		% $$p(z | y) = \iint p(z | u, f) p(f | u, y)df du,$$
		as $p(z|u, f, y) = p(z|u, f)$. 

		Suppose for a moment, that $u$ is a sufficient statistics for the parameter $f$ in the scence that $z$ and $f$ are conditionally independent given $u$. Then we have 
		$$p(z|f, u) = \frac {p(z, f|u)} {p(f|u)} = \frac {p(z | u) p(f | u)}{p(f|u)} = p(z|u),$$
		$$p(z|y, u) = \frac {p(z, y, u)}{p(y, u)} = \frac {\int p(y|f)p(f, z, u) du}{\iint p(y|f) p(f, z, u) df dz} = \frac {\int p(y|f) p(z|u) p(u|f) p(f)df}{\iint p(y|f) p(z|u) p(u|f) p(f)df dz} = $$
		$$= \frac {\int p(y|f)p(f)p(u|f)df \cdot p(z|u)} {\int p(y|f)p(f)p(u|f)df \cdot \int p(z|u) dz} = \frac{\int p(y, f) p(u|f) df} {\int p(y, f) p(u|f) df} p(z|u) = p(z|u).$$

		So, $p(z|y, u) = p(z|u)$. If we set the points, corrwsponding to the process values $z$, to the traing points, we will have $z = f$, and thus $p(f|y, u) = p(f|u)$.

		Substituting these formulas into (\ref{predictive1}) we achieve
		$$q(z) = p(z|y) = \iint p(z|u) p(f|u) p(u|y)df du = \iint p(z|u) p(u|y) du = $$
		\begin{equation}
			\label{predictive2}
			= \int p(z|u)\varphi(u) du  = \int q(z, u) du, 
		\end{equation}
		where $\varphi(u) = p(u|y)$, $q(z, u) = p(z|u)\varphi(u)$.

		In practice however it's difficult to guarantee that $u$ is a sufficient statistics. Thus we can only expect $q(z)$ to be an approximation to $p(z|y)$. In such case we can choose $\varphi(u)$ to be a variational distribution, where in general $\varphi(u) \ne p(u | y)$. We will consider $\varphi(u)$ to be Gaussian with a mean vector $\mu$ and covariance matrix $\Sigma$.

		By using the eq. (\ref{predictive2}) we can calculate the approximate posterior GP mean at point $x$ and covariance at points $x, x'$
		$$\E[z(x)] = K_{xm} K_{mm}^{-1} \mu,$$ 
		$$\cov(z(x), z(x')) = k(x, x') - K_{xm} K_{mm}^{-1} K_{mx'} + K_{xm} A K_{mx'},$$
		where $A = K_{mm}^{-1} \Sigma K_{mm}^{-1}$.

		Now we have to specify a way to find the variational distribution parameters $\mu$ and $\Sigma$, and the inducing input positions $X_m$ and a way to optimize the kernel hyper-parameters. 
		% In order to do so, we will form the variational distribution $q(f)$ and the exact posterior $p(f|y)$ on the training function values, and then minimize the distance between this two distributions. Equivalently, we can minimize a distance, between the augmented true posterior $p(f, u|y)$ and $q(f, u)$.
		In order to do so, we will form the variational distribution $q(f, u)$ and the exact posterior $p(f, u|y)$ on the training function values and the values at the inducing points, and then minimize the KL-divergence between these two distributions. This minimization is equivalently expressed as the maximization of the following lower bound of the true marginal likelihood:
		$$F_V(X_m, \varphi) = \iint p(f|u) \varphi(u) \log \frac{p(y|f) p(u)}{\varphi(u)} df du.$$
		This bound can be optimized analytically with respect to $\phi$. The optimal distribution $\varphi(u) \sim \N(u|\hat u, \Lambda^{-1})$, where
		$$\Lambda = \frac 1 {\sigma_n} K_{mm}^{-1} K_{mn} K_{nm} K_{mm}^{-1} + K_{mm}^{-1},$$
		$$\hat u = \frac 1 {\sigma_n} \Lambda^{-1} K_{mm}^{-1} K_{mn} y.$$
		Substituting the optimal values of variational parameters into the $F_V$ we obtain the following bound
		$$F_V(X_m) = \log \N(y|0, \sigma_n^2 I + K_{nm} K_{mm}^{-1} K_{mn}) - \frac 1 {2\sigma_n^2} \tr(\tilde K).$$

		Another derivation of this lower bound is provided in section (\ref{svi}).

		The bound $F_V(X_m)$ is computed in $o(nm^2)$ time. Now we will calculate it's gradient in order to be able to maximize it with respect to $X_m$ and kernel hyper-parameters. We will denote $B = \sigma_n^2 I + K_{nm} K_{mm}^{-1} K_{mn}$. Then
		$$F_V(X_m, \theta, \sigma_n) = -\frac 1 2 \left(n \log 2\pi + \log |B| + y^T B^{-1} y + \frac 1 {\sigma_n^2} \tr(\tilde K)\right),$$
		$$\derivative{F_V}{\theta} = \frac 1 2 \left( -\tr \left(B^{-1} \derivative{B}{\theta}\right) + y^T B^{-1} \derivative{B}{\theta} B^{-1} y - \right.$$    
		$$- \left. \frac 1 {\sigma_n^2} \tr\left(\derivative{K_{nn}}{\theta} - \left(\derivative{K_{nm}}{\theta}K_{mm}^{-1} - K_{nm} K_{mm}^{-1} \derivative{K_{mm}}{\theta}K_{mm}^{-1}\right) K_{mn} - K_{nm} K_{mm}^{-1} \derivative{K_{mn}}{\theta}\right)\right),$$
		where
		$$\derivative{B}{\theta} =\left(\derivative{K_{nm}}{\theta}K_{mm}^{-1} - K_{nm} K_{mm}^{-1} \derivative{K_{mm}}{\theta}K_{mm}^{-1}\right) K_{mn} +  K_{nm} K_{mm}^{-1} \derivative{K_{mn}}{\theta}.$$

		We can rewrite
		$$\derivative{F_V}{\theta} = \frac 1 2 \left( -\tr \left(B^{-1} \derivative{B}{\theta}\right) + y^T B^{-1} \derivative{B}{\theta} B^{-1} y - \frac 1 {\sigma_n^2} \tr \left(\derivative {K_{nn}} {\theta} - \derivative {B}{\theta}\right) \right).$$

		Now we can optimize $F_V$ with respect to kernel hyper-parameters. Similarly, we can take derivatives with respect to $X_m$ and $\sigma_n$ and opptimize $F_V$ with respect to them as well.

		However, if we compute $F_v$ and it's derivatives as they are, it takes $O(n^3)$ time which is not faster, than recovering the full Gaussian process. So, we have to rewrite these values in a form that allows for faster computation.

		First of all, let's deal with $\log|B|$ and $B^{-1}$. Using the matrix determinant lemma we obtain
		$$|B| = |\sigma_n^2 I + K_{nm} K_{mm}^{-1} K_{mn}| = \frac{\left|K_{mm} + \cfrac{K_{mn} K_{nm}}{\sigma_n^2}\right| \sigma_n^2}{|K_{mm}|}.$$
		So, denoting $A = K_{mm} + \cfrac{K_{mn} K_{nm}}{\sigma_n^2}$, we obtain
		$$\log |B| = \log |A| + 2 \log \sigma_n - \log |K_{mm}|.$$
		Note tha this is computed in $O(n m^2)$ instead of $O(n^3)$.

		Using the Woodbury identity, we obtain
		$$B^{-1} = (\sigma_n^2 I + K_{nm} K_{mm}^{-1} K_{mn})^{-1} = \frac I {\sigma_n^2} - \frac{K_{nm} A^{-1} K_{mn}}{\sigma^{4}},$$
		which allows for computing $y^T B^{-1} y$ in $O(n m)$.

		Similarly, we can compute the gradient in $O(nm^2)$. In order to do so, we need to rewrite every trace $\tr(M_{nm} M_{mm} M_{mn})$, where $M_{kl} \in \R^{k \times l}$, in the form $\tr(M_{mm} M_{mn} M_{nm})$, which is computed in $O(nm^2)$, and use the derived formulas for $B^{-1}$.


	\pagebreak
	\subsubsection{Stochastic variational inference}
		\label{svi}
	
		\hspace{0.6cm}The method discussed here was proposed in \cite{BigData}. The method doesn't provide a way to choose the positions of inducing points. It provides a way to find the predictive distribution and optimize hyper-parameters for large datasets.
		
		For using stochastic variational inference, we have to provide a lower bound for the marginal likelihood, that factorizes over the training examples. To obtain such an ELBO (evidence lower bound) two ancillary lower bounds are found. 
		
		By applying the Jensen inequality we obtain
		$$ \log p(y | u) = \log \left ( \int p(y|f) p(f | u) du\right) \ge  \int \log (p(y|f)) p(f | u) du = L_1.$$	
		As $p(y | f)$ factorizes over examples we obtain
		$$\exp(L_1) = \prod_{i = 1}^{n} \N(y_i| \mu_i, \sigma_n^2) \exp \left( -\frac 1 {2\sigma_n^2} \tilde K_{ii}\right).$$
		Note that 
		$$\log p(y | u) - L_1 = \KL{p(f | u)}{p(f | u, y)}.$$
		
		Using the lower bound $L_1$ we obtain a lower bound for the marginal likelihood
		$$\log p(y) = \log \left( \int p(y|u) p(u) du\right) \ge \log \left( \int \exp(L_1) p(u) du\right) = L_2.$$
		With some algebraic manipulations we obtain the following expression for $L_2$
		$$L_2 = \log \N(y| 0, K_{nm} K_{mm}^{-1} K_{mn} + \sigma_n^2 I) - \frac 1 {2 \sigma_n^2} \tr (\tilde K).$$
		This is exactly the expression for the lower bound, used in the method, described in the section \ref{Titsias} for the optimal approximating distribution $q(u) = \N(u |\hat u, \Lambda^{-1}),$ where
		$$\Lambda = \frac 1 {\sigma_n^2} K_{mm}^{-1} K_{mn} K_{nm} K_{mm}^{-1} + K_{mm}^{-1},$$
		$$\hat u = \frac 1 {\sigma_n^2} \Lambda^{-1} K_{mm}^{-1} K_{mn} y.$$
		In the method, described in section \ref{Titsias}, this lower bound is being maximized over the kernel hyper-parameters and the optimal distribution $q(u)$ is used for making predictions at unseen points $x$ as follows
		$$\E f(x) = K_{xm} K_{mm}^{-1} \hat u, $$
		$$\cov(f(x), f(x')) = k(x, x') - K_{xm} K_{mm}^{-1} K_{mx'} + K_{xm} K_{mm}^{-1} \Lambda^{-1} K_{mm}^{-1} K_{mx'}.$$
		
		Unfortunately, evaluating $\Lambda$ takes $O(n m^2)$ operations and thus this method cannot be applied to big datasets. To overcome this limitation, we will use stochastic optimization to find the approximate optimal distribution $q(u)$ and to optimize for hyper-parameters. 
		
		Let the variational distribution $q$ be normal with mean $\mu$ and covariance matrix $\Sigma$. The final ELBO is derived as follows
		$$\log p(y) \ge \int \left( L_1 + \log p(u) - \log q(u)\right) q(u) du = L_3.$$
		This lower bound factorizes over the examples 
		$$L_3 = \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr (\frac 1 {\sigma_n^2} \Sigma K_{mm}^{-1} k_i k_i^T K_{mm}^{-1}) \right) - \KL{q(u)}{p(u)} = $$
		$$ = \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr (\Sigma \Lambda_i) \right) - $$
		$$ -\frac 1 2 \left (\log \frac {|K_{mm}|} {|\Sigma|} - m + \tr(K_{mm}^{-1} \Sigma) + \mu^T K_{mm}^{-1} \mu \right),$$
		where $\Lambda_i = \frac 1 {\sigma_n^2} K_{mm}^{-1} k_i k_i^T K_{mm}^{-1}$, and $k_i$ is the $i$-th column of the matrix $K_{mn}$.
		
		In stochastic variational inference natural gradients are used to maximize the ELBO. The canonical parameters for the normal distribution $q(u)$ are
		$$\eta_1 = \Sigma^{-1} \mu, \hspace{0.3cm}\eta_2 = - \frac 1 2 \Sigma^{-1}.$$
		The expectation parameters are
		$$\beta_1 = \mu, \hspace{0.3cm}\beta_2 = \mu \mu^T + \Sigma.$$
		In the exponential family the natural gradients are equal to the gradients with respect to expectation parameters. To find these gradients we first reparametrise the ELBO
		$$L_3(\beta_1, \beta_2) =  \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \beta_1, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr ((\beta_2 - \beta_1 \beta_1^T) \Lambda_i) \right) - $$
		$$ -\frac 1 2 \left (\log |K_{mm}| - \log |\beta_2 - \beta_1 \beta_1^T| - m + \tr(K_{mm}^{-1} (\beta_2 - \beta_1 \beta_1^T)) + \beta_1^T K_{mm}^{-1} \beta_1 \right).$$
		Differentiating with respect to expectation parameters we obtain
		\begin{equation}
			\label{natgrad1}
			\frac{\partial L_3} {\partial\beta_1} =  -\frac 1 {\sigma_n^2}  \sum_{i = 1}^{n} \left(K_{mm}^{-1} k_i y_i \right) + \Sigma^{-1} \mu,
		\end{equation}
		\begin{equation}
			\label{natgrad2}
			\frac{\partial L_3} {\partial\beta_2} = \frac 1 {2} \left(-\sum_{i = 1}^{n} (\Lambda_i) + \Sigma^{-1}  - K_{mm}^{-1}\right).
		\end{equation}
		
		The natural gradient descent updates of these parameters are
		$$\eta_{1(t+1)} = \Sigma_{(t+1)}^{-1} \mu_{(t+1)} = \Sigma_{(t)}^{-1} \mu_{(t)} + \ell \left(\frac 1 {\sigma_n^2} K_{mm}^{-1} K_{mn} y - \Sigma_{(t)}^{-1} \mu_{(t)} \right), $$
		$$\eta_{2 (t+1)} = -\frac 1 2 \Sigma_{(t+1)}^{-1} = -\frac 1 2 \Sigma_{(t)}^{-1}  + \ell \left( -\frac 1 2 \Lambda + \frac 1 2 \Sigma_{(t+1)}^{-1}\right),$$
		where $\ell$ is the step length. It's easy to see, that if $\ell = 1$ the method converges to the optimal distribution $q(u)$ in one iteration. Unfortunately, we can not directly compute the updates described above, because the computational complexity of computing the matrix $\Lambda$ is $O(n m^2)$. We will use approximations to the natural gradients, obtained by considering the data points individually or in batches. The formulas for these approximations can be obtained from equalities \ref{natgrad1}, \ref{natgrad2}.
		
		Finally, we need to find the derivatives of the ELBO with respect to kernel hyper-parameters $\theta$ apart from $\sigma_n^2$
		$$\frac{\partial L_3} {\partial \theta} = \sum_{i = 1}^n \left [ \frac 1 {\sigma_n^2} (y_i - k_i^T K_{mm}^{-1} \mu) \left(\frac{\partial k_i^T}{\partial \theta} K_{mm}^{-1} - k_i^T K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta} K_{mm}^{-1} \right)\mu + \right.$$
		$$\left. +\frac 1 {2 \sigma_n^2} \left (- \frac{\partial K_{nn}}{\partial \theta} +  \frac{\partial K_{nm}}{\partial \theta} K_{mm}^{-1} K_{mn} + K_{nm} K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta} K_{mm}^{-1} K_{mn} + K_{nm} K_{mm}^{-1} \frac{\partial K_{mn}}{\partial \theta}\right)_ {ii} + \right.$$
		$$\left. +  \frac 1 {\sigma_n^2} \tr\left( \Sigma \left( K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta} K_{mm}^{-1} k_i k_i^T K_{mm}^{-1}  -  K_{mm}^{-1} \frac{\partial k_{i}}{\partial \theta}k_i^T K_{mm}^{-1}\right)\right)\right] - $$

		$$ - \frac 1 2 \tr\left(K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta}\right) + \frac 1 2 \tr\left(\Sigma K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta} K_{mm}^{-1} \right)+ \frac 1 2 \mu^T K_{mm}^{-1} \frac{\partial K_{mm}}{\partial \theta} K_{mm}^{-1}\mu,$$
		and for $\sigma_n$ we have the same formula plus the following correction
		$$\sum_{i = 1}^{n} \left(-\frac{1}{\sigma_n} + \frac 1 {\sigma_n^3} (k_i^T K_{mm}^{-1} \mu - y_i)^2 + \frac 1 {\sigma_n^3} \tilde K_{ii} + \frac {\tr(\Sigma \Lambda_i)}{\sigma_n}\right).$$
		Now, we can optimize the kernel hyper-parameters and the noise variance alongside the variational parameters. 
		
		We can also maximize the $L_3$ with procedures, other than stochastic gradient descent. However, in most of the effective optimization methods we can't use natural gradients, because they are not necesserily a descending direction. Thus, we have to use the usual gradients. However, there is a problem with this approach as well. The steps in the direction of the antigradient does not guarantee that the updated covariance $\Sigma$ is positive definite. 

		To solve this problems, we use Choletsky decomposition $L_{\Sigma}$ of $\Sigma$ and optimize $L_3$ with respect to it.
		$$L_3(L_{\Sigma}, \mu) = \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr (L_{\Sigma} L_{\Sigma}^T \Lambda_i) \right) - $$
		$$ -\frac 1 2 \left (\log \frac {|K_{mm}|} {|L_{\Sigma} L_{\Sigma}^T|} - m + \tr(K_{mm}^{-1} L_{\Sigma} L_{\Sigma}^T) + \mu^T K_{mm}^{-1} \mu \right) = $$
		$$ = \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr (L_{\Sigma}^T \Lambda_iL_{\Sigma}) \right) - $$
		$$ -\frac 1 2 \left (\log |K_{mm}| - 2 \sum_{j=1}^{m}\log (L_{\Sigma})_{jj} - m + \tr(L_{\Sigma}^T K_{mm}^{-1} L_{\Sigma}) + \mu^T K_{mm}^{-1} \mu \right)$$

		The gradients with respect to $\mu$ and $L_{\sigma}$ are given by
		$$
		\derivative{L_3}{\mu} =  \sum_{i=1}^n \left(\Lambda_i \mu - \frac {y_i}{\sigma_n^2} K_{mm}^{-1} k_i \right) + K_{mm}^{-1} \mu,
		$$
		$$\derivative{L_3}{L_{\Sigma}} = - \sum_{i=1}^n \Lambda_i L_{\Sigma} +
		\left(
		\begin{array}{cccc}
		\frac 1 {(L_{\Sigma})_{11}} & 0 & \ldots & 0\\
		0 & \frac 1 {(L_{\Sigma})_{22}} & \ldots & 0\\
		\ldots & \ldots & \ldots & \ldots\\
		0 & 0 & \ldots & \frac 1 {(L_{\Sigma})_{mm}} \\
		\end{array}   
		\right) 
		- K_{mm}^{-1} L_{\Sigma}.
		$$

\pagebreak
\begin{thebibliography}{99}

\bibitem{Titsias}
Titsias M. K. (2009).  Variational Learning of Inducing Variables in Sparse Gaussian
Processes.  In: {\it International Conference on Artificial Intelligence and Statistics}, pp.~567–574.

\bibitem{BigData}
Hensman J., Fusi N., Lawrence D. (2013).  Gaussian Processes for Big Data.  In: {\it Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence}.

\end{thebibliography}	
\end{document}