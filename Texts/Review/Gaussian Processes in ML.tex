\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{textcomp}
\usepackage{a4wide}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subfig}
\usepackage{listings}
\usepackage{hyperref}
% \usepackage{fontspec}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{pgf,pgfarrows,pgfnodes}
\usepackage{pgf}

\lstset{
language=Python,
basicstyle=\ttfamily\small,
otherkeywords={self},                   
}

\title{Title}
\title{Неточный метод Ньютона.}
\date{4 октября 2015}
\author{Павел Измаилов}

\begin{document}

\renewcommand{\contentsname}{\centerline{\bf Contents}}
\renewcommand{\refname}{\centerline{\bf Literature}}

\newcommand{\GP}{\mathcal{GP}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\cov}{\mbox{cov}}
\newcommand{\Nystrom}{Nystr\"{o}m }
\newcommand{\KL}[2]{\mbox{KL}\left(#1\mbox{ || }#2\right)}
\newcommand{\tr}{\mbox{tr}}
\newcommand{\derivative}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sndderivative}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\bigO}{\mathcal{O}}

\newlength{\arrayrulewidthOriginal}
\newcommand{\Cline}[2]{%
  \noalign{\global\setlength{\arrayrulewidthOriginal}{\arrayrulewidth}}%
  \noalign{\global\setlength{\arrayrulewidth}{#1}}\cline{#2}%
  \noalign{\global\setlength{\arrayrulewidth}{\arrayrulewidthOriginal}}}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}


\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}} {\mbox{\boldmath$\scriptscriptstyle#1$}}}

%\maketitle
\centerline{Lomonosov Moscow State University}

\centerline{Faculty of Computer Science}

\vspace{5 cm}

\centerline{\Large Review of matherials on}

\vspace{1 cm}

\centerline{\Large \bf Gaussian Processes for Machine Learning}

\vspace{6 cm}

\begin{flushright}

Pavel Izmailov
\end{flushright}

\vfill 

\centerline{Moscow,  2016}
\thispagestyle{empty} 
\pagebreak

\section{Theory}

In this section an introduction to Gaussian process theory is provided.

\subsection{Gaussian Process}
	\input{introduction.tex}
	
\subsection{GP-regression}
	\input{gp_regression.tex}
		
\subsection{GP-classification}
	\input{gp_classification.tex}
	
\subsection{Kernel functions}
	To be wrritten.
	
\subsection{Hyper-parameter estimation}
	\input{hyperparameter_estimation.tex}
	
\subsection{Theoretical perspectives}
	\hspace{0.6cm}To be wrritten.

\pagebreak
\section{Review of existing methods}
	\input{methods.tex}
	\pagebreak

\section{Experiments}
	\input{experiments.tex}

\pagebreak
\begin{thebibliography}{99}

	\bibitem{Titsias}
	Titsias M. K. (2009).  Variational Learning of Inducing Variables in Sparse Gaussian
	Processes.  In: {\it International Conference on Artificial Intelligence and Statistics}, pp.~567–574.

	\bibitem{BigData}
	Hensman J., Fusi N., Lawrence D. (2013).  Gaussian Processes for Big Data.  In: {\it Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence}.

	\bibitem{SVIclassification}
	Hensman J., Matthews G., Ghahramani Z. (2015). Scalable Variational Gaussian Process Classification.  In: {\it Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics}.

	\bibitem{JaakkolaJordan}
	Jaakkola T., Jordan M. (1996). A Variational Approach to Bayesian Logistic Regression Models and Their Extensions. In: {\it Artificial Intelligence and Statistics}.

\end{thebibliography}	
\end{document}