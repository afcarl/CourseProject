In this section the results of the numerical experiments are provided. All of the provided plots has a title, that tells the number of training points $n$, the number of features $d$ and the number of inducing points $m$. The title also tells the name of the dataset.

	The methods were compared on variaus datasets. Some of them are generated from a gaussian process and others are real. The $R^2$-score on a test set was used as a quality metric. 

	The squared exponential kernel was used in all the experiments.

	\subsection{Variations of the stochastic variational inference method}
		In this section we compare several variations of the stochastic variational inference method.

		The first variation is denoted by \lstinline{svi-natural}. It is the method described in \cite{BigData}. It uses stochastic gradient descent with natural gradients for minimizing the ELBO with respect to the variational parameters, and usual gradients with respect to kernel hyperparameters.

		The methods \lstinline{svi-L-BFGS-B} and \lstinline{svi-FG} use the full (non-stochastic) ELBO from the same article \cite{BigData} and minimize it with L-BFGS-B and gradient descent respectively. These methods use Cholesky factorization (see \ref{svi}) for the variational parameters.

		Finally, the \lstinline{svi-SAG} method to minimize the ELBO. This method also uses Cholesky factorization.
		We will compare the methods on datasets, generated from some gaussian process and on real data. 

		% The generated dataset consisted of $500$ train points with $2$ features. $100$ inducing inputs were used 

		The results on small and medium datasets are shown in fig. \ref{svi_small} and fig. \ref{svi_medium}.

		\begin{figure}[h!]
			\centering

			\subfloat{
				\scalebox{0.75}{
					\input{../../Code/Experiments/Plots/svi_variations/small_generated.pgf}
				}
			}
			\subfloat{
				\scalebox{0.75}{
		    		\input{../../Code/Experiments/Plots/svi_variations/small_real.pgf}
				}
			}
			\vspace{0.1cm}
			\caption{Svi methods' performance on small datasets}
			\label{svi_small}
		\end{figure}


		\begin{figure}[h!]
			\centering
			\subfloat{
				\scalebox{0.75}{
					\input{../../Code/Experiments/Plots/svi_variations/medium_generated.pgf}
				}
			}
			\subfloat{
				\scalebox{0.75}{
		    		\input{../../Code/Experiments/Plots/svi_variations/medium_real.pgf}
				}
			}
			\label{svi_medium}
			\caption{Svi methods' performance on medium datasets}
		\end{figure}

\subsection{Comparison of stochastic and non-stochastic variational inference methods}
	In this section we compare the \lstinline{vi-means} method with \lstinline{svi-L-BFGS-B}. The \lstinline{vi-means} method is a variation of the method, described in section \ref{Titsias}. It does not optimize for the inducing point positions and does uses \lstinline{L-BFGS-B} to maximize the ELBO.

	\begin{figure}[!h]
		\centering
		\subfloat{
			\scalebox{0.75}{
				\input{../../Code/Experiments/Plots/vi_vs_svi/small_real.pgf}
			}
		}
		\subfloat{
			\scalebox{0.75}{
	    		\input{../../Code/Experiments/Plots/vi_vs_svi/medium_real.pgf}
			}
		}
		
		\caption{Method's performance on small and medium datasets}
	\end{figure}

	\begin{figure}[!h]
		\centering
		\subfloat{
			\scalebox{0.75}{
		    	\input{../../Code/Experiments/Plots/vi_vs_svi/big_real.pgf}
			}
		}
		\caption{Method's performance on a big dataset}
	\end{figure}

	We can see, that \lstinline{vi-means} beats it's oponent in all the experiments. One could expect these results, because \lstinline{vi-means} optimizes the exact same functional as it's oponent, but it uses exact optimal values for some of the parameters. Thus, on moderate problems the \lstinline{vi-means} method beats all the discussed \lstinline{svi} variations.

	Finally, we will compare \lstinline{vi-means} with stochastic \lstinline{svi-natural} an \lstinline{svi-SAG} on a big dataset. The results can be found in fig. \ref{visvi_big}.

	\begin{figure}[!h]
		\centering
		\subfloat{
			\scalebox{0.73}{
				\input{../../Code/Experiments/Plots/vi_vs_svi/1e5_sg_lbfgs.pgf}
			}
		}
		\caption{vi and svi methods comparison on a big dataset}
		\label{visvi_big}
	\end{figure}

\subsection{Variations of variational inference method}
	In this section we compare several variations of the stochastic variational inference method. The method itself is described in section \ref{Titsias}. We compare two different optimization methods for minimizint the Titsias's ELBO.

	The first variation is denoted by \lstinline{means-PN}. It uses Projected-Newton method for minimizing the ELBO. The second variation is denoted by \lstinline{means-L-BFGS-B} and uses L-BFGS-B optimization method.

	The \lstinline{means-PN} uses finite-difference approximation of the hessian. It also makes hessian-correction in order to make it simmetric positive-definite.

	We compare the methods on several different datasets. The results on a small and medium datasets can be found in fig. \ref{vi_small}. The results on a biger dataset can be found in fig. \ref{bi_big}

	\begin{figure}[!h]
		\centering
		\subfloat{
			\scalebox{0.75}{
				\input{../../Code/Experiments/Plots/vi_variations/small_real.pgf}
			}
		}
		\subfloat{
			\scalebox{0.75}{
	    		\input{../../Code/Experiments/Plots/vi_variations/medium_real.pgf}
			}
		}

		\label{vi_small}
		\caption{Method's performance on small and medium datasets}
	\end{figure}
	\begin{figure}[!h]
		\centering
		\subfloat{
			\scalebox{0.75}{
				\input{../../Code/Experiments/Plots/vi_variations/big_real.pgf}
			}
		}
		\subfloat{
			\scalebox{0.75}{
				\input{../../Code/Experiments/Plots/vi_variations/huge_real.pgf}
			}
		}
		\label{vi_big}
		\caption{Method's performance on a bigger dataset}
	\end{figure}

\subsection{\lstinline{vi-classification} and \lstinline{svi-classification} methods for classification}
	In this section we compare \lstinline{vi-means} and \lstinline{svi} approaches to the classification problem