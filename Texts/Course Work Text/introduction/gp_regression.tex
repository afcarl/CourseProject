\label{gp_regression}
Consider the regression problem. We have a dataset $\{(x_i, f_i) | i = 1, \ldots, n\}$, which is considered to be generated from an unknown Gaussian process $f \sim \GP(m(\cdot), k(\cdot, \cdot))$, let $x \in \R^d$.  We will denote the matrix comprised of points $x_1, \ldots, x_n$ by $X \in \R^{n \times d}$ and the vector of corresponding target values $f_1, ..., f_n$ by $f \in \R^n$. We want to predict the values $f_* \in \R^l$ of the unknown process at a set of other $l$ points $X_* \in \R^{l \times d}$.

We will consider the case, when we can't directly observe the values $f$ of the process at points $X$. Instead, we will use their noisy versions $y$, which are distributed as
$$y \sim \N(f, \nu^2 I),$$
for some noise variance $\nu \in \R$. If $\nu = 0$ we obtain the noise-free case $y = f$.

We now put a zero-mean Gaussian process prior on our model
$$f \sim \GP(0, k(\cdot, \cdot)).$$
This prior might seem limiting, however a zero-mean prior does not imply a zero-mean predictive distribution.

The probabilistic model for this setting is given by
$$p(y, f | X) = p(y | f, X) p(f | X).$$
In the following text we will omit $X$ in order to make the formulas simpler. We then rewrite the model in the following form
\begin{equation}\label{standard_model}
	p(y, f) = p(y|f) p(f) = p(f) \prod_{i = 1}^n p(y_i | f_i) .
\end{equation}
The graphical model for this setting is provided in fig. \ref{gp_graphical_model}.

We will use the following notaion. We will denote the matrix, comprised of pairwise values, of covariance functions on two sets of points $A = (a_1, \ldots, a_n)^T \in \R^{n \times d}$ and $B = (b_1, \ldots, b_m)^T \in \R^{m \times d}$ by
$$K(A, B) = 
\left (\begin{array}{cccc} 
	k(a_1, b_1) & k(a_1, b_2) & \ldots & k(a_1, b_m) \\
	k(a_2, b_1) & k(a_2, b_2) & \ldots & k(a_2, b_m) \\
	\ldots & \ldots & \ldots & \ldots \\
	k(a_n, b_1) & k(a_n, b_2) & \ldots & k(a_n, b_m) \\
\end{array} \right) \in \R^{n \times m}.
$$

Then, by definition of a Gaussian process the joint distribution of $f$ and $f_*$ is given by
$$
\left [ \begin{array}{c} f\\ f_* \end{array} \right ]
\sim
\N \left ( 0, \left [\begin{array}{cc} K(X, X) & K(X, X_*)\\ K(X_*, X) & K(X_*, X_*) \end{array} \right] \right ).
$$

\begin{figure}[!t]
	\centering
	\scalebox{0.9}{
		\input{../pictures/full_gp_gm.tikz}
	}
	\caption{The graphical model for Gaussian process regression and classification}
	\label{gp_graphical_model}
\end{figure}

As $y$ is obtained by adding a normally distributed noise to $f$, the joint distribution

$$
\left [ \begin{array}{c} y\\ f_* \end{array} \right ]
\sim
\N \left ( 0, \left [\begin{array}{cc} K(X, X) + \nu^2 I & K(X, X_*)\\ K(X_*, X) & K(X_*, X_*) \end{array} \right] \right ).
$$

Conditioning this distribution, we obtain the predictive
$$f_* | y \sim \N( \hat m, \hat K ),$$
where
$$\E[f_* | y] = \hat m = K(X_*, X) (K(X, X) + \nu^2 I)^{-1} y,$$
$$\cov(f_* | y ) = \hat K = K(X_*, X_*) - K(X_*, X)(K(X, X) + \nu^2 I)^{-1}K(X, X_*).$$

Thus, the complexity of obtaining the predictive distribution is determined by the complexity of inverting the $n$ by $n$ matrix $K(X, X) + \nu^2 I$ and thus scales as $\bigO(n^3)$.

% \begin{figure}[!h]
% 	\centering
% 	\subfloat{
% 		\scalebox{0.7}{
% 			\input{../../Code/Experiments/pictures/1dgp-regression.pgf}
% 		}
% 	}
% 	% \subfloat{
% 	% 	\scalebox{0.7}{
%  %    		\input{../../Code/Experiments/pictures/2dgp-regression.pgf}
% 	% 	}
% 	% }
% 	\caption{One and two-dimensional Gaussian processes, reconstructed from data}
% 	\label{brute_reg_example}
% \end{figure}


In fig. \ref{cov_examples} you can see examples of one-dimensional Gaussian processes, reconstructed from data. The data points are shown by black `$+$' signs. 

For more detailed description of Gaussian process regression see \cite{GPinML}.