\begin{figure}[!t]
	\centering
	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/d1_n500.pgf}
		}
	}
	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/d5_n500.pgf}
		}
	}

	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/d10_n4000.pgf}
		}
	}
	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/abalone.pgf}
		}
	}
	\caption{The dependence between prediction quality and the number of inducinge inputs for the regression problem}
	\label{ind_points_results}
\end{figure}


We've seen above, that inducing input methods have a much smaller computational complexity then the standard methods for both GP-regression and GP-classification problems. In this section we empirically compare the prediction quality on the test data for these methods with the quality obtained by the standard methods.

We also explore the dependence between the number of inducing points used by the method and the prediction quality.

For the regression problem we use two variations of the \lstinline{vi} method. The \lstinline{vi-means} method does not maximize the lower bound with respect to the positions of the inducing inputs and just uses the K-Means cluster centers as the positions of the inducing inputs. The \lstinline{vi} method on the other hand does optimize for the inducing input positions. \lstinline{full GP} method is the standard GP-regression method, described in section \ref{gp_regression}.

In the two top plots of fig. \ref{ind_points_results} the dependence between the number of inducing points and the prediction quality is shown on two small datasets.  As we can see, the optimization with respect to the positions of the inducing inputs does not dramatically increase the quality of the predictions. It does however make the optimization problem that we have to solve much harder and makes the method much slower. We will thus abandon this method and only use \lstinline{vi-means} method in other experiments.

We can also see from these plots that for sufficient number of inducing inputs the \lstinline{vi} methods reach the predictive quality of the standard methods. 

In the bottom plots of fig. \ref{ind_points_results}, the dependence between the number of used inducing inputs and the quality of the \lstinline{vi-means} method predictions is shown for two bigger datasets. 

\begin{figure}[!t]
	\centering
	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/class_d2_n200.pgf}
		}
	}
	\subfloat{
		\scalebox{0.8}{
			\input{../../Code/Experiments/plots/inducing_inputs/class_german.pgf}
		}
	}
	\caption{The dependence between prediction quality and the number of inducinge inputs for the classification problem}
	\label{ind_inputs_class_results}
\end{figure}

In fig. \ref{ind_inputs_class_results} the results of similar experiments for the classification problem are provided. Here we compare the \lstinline{Laplace} method, which was described in section \ref{gp-classification}, and the \lstinline{svi-classification} method, which was described in section \ref{svi_classification}.

As we can see, on the chosen dataset the \lstinline{svi-classification} method can not reach the predictive quality of the \lstinline{Laplace} method even for reasonably big values of $m$. However, the time consumption of the \lstinline{Laplace} method does not allow to use it even for moderate~problems. 



