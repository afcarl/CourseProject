In this subsection we describe a method for maximizing the lower bound (\ref{main_elbo}) in case of the GP-regression problem, which was proposed in \cite{BigData}. While the method, described in the previous section is much faster then the full GP-regression, it's complexity is still rather big. We could try, to reduce the time consumption of optimizing the lower bound, by using stochastic optimization methods. However, the function in the right hand side of (\ref{titsias_elbo}) does not have a form of sum over objects, and thus it's not clear, how to apply the stochastic methods.

However, the original bound from (\ref{main_elbo}) does have a form of sum over objects, and we can thus apply stochastic methods to it. In the regression case the bound looks like

$$\log p(y) \ge \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \sigma_n^2) - \frac 1 {2 \sigma_n^2} \tilde K_{ii} - \frac 1 2 \tr (\Sigma \Lambda_i) \right) - $$

\begin{equation} \label{svi_elbo}
	-\frac 1 2 \left (\log \frac {|K_{mm}|} {|\Sigma|} - m + \tr(K_{mm}^{-1} \Sigma) + \mu^T K_{mm}^{-1} \mu \right).
\end{equation}

In the \lstinline{svi} method, we directly optimize this ELBO with respect to both variational parameters and kernel hyper-parameters in a stochastic way. The authors of the method suggest to use the stochastic gradient descent with natural gradients for the variational parameters and usual gradients for kernel hyper-parameters.

The natural gradients are the gradients with respect to the natural parameters of an exponential family of distributions. These gradients are considered to be effective in the case of optimization with respect to probability distribution parameters, because they use symmetrized $\mbox{KL}$ divergence between the distributions instead of usual distance between distribution parameters as a distance metric. For more information about natural gradients see for example \cite{ExpFamilyGeom}.

The complexity of computing a stochastic update of the variational and kernel parameters is independent of $n$ and scales as $\bigO(m^3)$. Thus, the stochastic optimization might give this method advantage against the \lstinline{vi} method. However, for big data problems the number of required inducing points $m$ is usually quite big. The number of parameters we have to optimize scales as $\bigO(m^2)$, which makes the optimization problem of the \lstinline{svi} method much harder then the one we have to solve in the \lstinline{vi} method. We will compare the two methods in the experiments section.
