In this subsection we describe a method for maximizing the lower bound (\ref{main_elbo}) in case of the GP-regression problem, which was proposed in \cite{BigData}. While the method described in the previous section is much faster then the full GP-regression, it's complexity is still rather big. We could try to reduce the time consumption of optimizing the lower bound by using stochastic optimization methods. However, the function in the right-hand side of (\ref{titsias_elbo}) does not have a form of sum over objects, and thus it's not clear, how to apply the stochastic methods.

However, the original bound from (\ref{main_elbo}) does have a form of sum over objects, and we can thus apply stochastic methods to it. In the regression case the expectations in the bound (\ref{main_elbo}) are tractable. In this case, we can rewrite the bound as

$$\log p(y) \ge \sum_{i = 1}^{n} \left( \log \N(y_i | k_i^T K_{mm}^{-1} \mu, \nu^2) - \frac 1 {2 \nu^2} \tilde K_{ii} - \frac 1 2 \tr (\Sigma \Lambda_i) \right) - $$

\begin{equation} \label{svi_elbo}
	-\frac 1 2 \left (\log \frac {|K_{mm}|} {|\Sigma|} - m + \tr(K_{mm}^{-1} \Sigma) + \mu^T K_{mm}^{-1} \mu \right),
\end{equation}
where $\Lambda_i = \frac 1 {\nu^2} K_{mm}^{-1} k_i k_i^T K_{mm}^{-1}$, and $k_i = K(x_i, Z)$ is the vector of covariances between the $i$-th data point and inducing points.

In the \lstinline{svi} method we directly optimize this ELBO with respect to both variational parameters and kernel hyper-parameters in a stochastic way. The authors of the method suggest to use the stochastic gradient descent with natural gradients for the variational parameters and usual gradients for kernel hyper-parameters.

Natural gradients are gradients with respect to the natural parameters of an exponential family of distributions. These gradients are considered to be effective in case of optimization with respect to probability distribution parameters, because they use symmetrized KL divergence between the distributions instead of usual distance between distribution parameters as a distance metric. For more information about natural gradients see, for example \cite{ExpFamilyGeom}.

The complexity of computing a stochastic update of variational and kernel parameters is independent of $n$ and scales as $\bigO(m^3)$. The complexity of one pass over data (epoch) is thus $\bigO(nm^3)$ which is worse, than the corresponding complexity of the \lstinline{vi} method. However, the stochastic optimization might give this method advantage against the \lstinline{vi} method, because stochastic optimization some times leads to faster convergence in big data problems. 

However, for big problems the number of required inducing points $m$ is usually quite big. The number of parameters we have to optimize scales as $\bigO(m^2)$. Indeed, we need to optimize the bound with respect to the variational parameters $\mu$ ($m$ parameters) and $\Sigma$ ($\frac {m(m + 1)} 2$ parameters), and with respect to kernel hyper-parameters. This makes the optimization problem of the \lstinline{svi} method much harder than the one we have to solve in the \lstinline{vi} method (where we only have to optimize the bound with respect to kernel hyper-parameters). We will compare the two methods in the experiments section.
