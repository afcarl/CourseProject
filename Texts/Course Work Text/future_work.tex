As we have seen, for the regression problem \lstinline{vi} method beats \lstinline{svi} in all of the provided experiments. We think that the main reason for that is the complexity of optimization with respect to variational distribution parameters $\mu$ and $\Sigma$. In the classification problem, there is no known analogy for the \lstinline{vi} method. Moreover, the  \lstinline{svi-classification} method does not use natural gradients with respect to variational parameters. This might lead to slow covergence, as natural gradients proved to be useful in our experiments.

This leads us to the idea, that if we could avoid the optimization with respect to variational parameters $\mu$ and $\Sigma$, we might be able to obtain a faster converging method. If we use the logistic likelihood function $\log p(y_i | f_i) = -(1 + \exp(-y_i f_i))$ for the classification, the problem of maximizing the lower bound is very similar to the Bayesian logistic regression problem. Paper \cite{JaakkolaJordan} provides a quadratic lower bound for the Bayesian logistic regression setting. Using this bound in the GP-classification problem would lead to analytical formulas for optimal $\mu$ and $\Sigma$. We will compare this approach to the \lstinline{svi-classification} approach.

We will also perform more experiments to determine whether using second-order \linebreak optimization for the \lstinline{vi} method is beneficial over L-BFGS-B.

Finally, in the \lstinline{svi-classification} method we can use the gradient of one sample $\log p(y_i | \tilde f)$ from the distribution $\tilde f \sim q(f_i)$ to onbtain an unbiased estimate of the gradient of the expectation $\E_{q(f_i)} \log p(y_i | f_i)$. Now we use Gauss-Hermite quadratures to approximate the expectation. This might lead to a faster version of the \lstinline{svi-classification} method.